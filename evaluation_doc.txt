FastAPI is a modern, fast web framework for building APIs with Python. It supports asynchronous programming and automatic documentation via OpenAPI. 

Retrieval-Augmented Generation (RAG) combines information retrieval with generative language models to produce context-aware answers. RAG retrieves relevant document chunks and feeds them to a language model for generating responses.

ChromaDB is a vector database that stores embeddings of text chunks. It enables fast similarity search and retrieval, making it ideal for semantic search tasks in AI applications.

FAISS is a library for efficient similarity search on dense vectors. It allows rapid nearest-neighbor search and is commonly used for embeddings generated by transformer models.

MiniLM is a lightweight sentence transformer model used for generating embeddings from text. It converts sentences or paragraphs into numerical vectors that capture semantic meaning.

FLAN-T5 is a text-to-text transformer model that can perform multiple NLP tasks such as summarization, translation, and question answering. It uses an encoder-decoder architecture.

Semantic search is the process of finding information based on meaning rather than keyword matching. It relies on embeddings and vector similarity to retrieve relevant content.

Document chunking divides large texts into smaller segments. Chunking helps in managing long documents efficiently and improves retrieval in RAG pipelines.

Vector databases store embeddings along with metadata, allowing fast similarity search. They support operations like inserting new embeddings, updating, and querying.

RAG APIs combine all these components: documents are chunked, embeddings are stored in a vector DB, and an LLM generates answers based on retrieved chunks.
