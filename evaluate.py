import time
from fuzzywuzzy import fuzz  # pip install fuzzywuzzy
from app.embedding import Embedder
from app.retriever import Retriever
from app.generator import Generator
import numpy as np

# Full evaluation dataset
evaluation_data = [
    {"question": "What is FastAPI?", 
     "expected_answer": "FastAPI is a modern, fast web framework for building APIs with Python. It supports asynchronous programming and automatic documentation via OpenAPI."},

    {"question": "What does RAG stand for?", 
     "expected_answer": "Retrieval-Augmented Generation (RAG) combines information retrieval with generative language models to produce context-aware answers."},

    {"question": "What is the main function of ChromaDB?", 
     "expected_answer": "ChromaDB is a vector database that stores embeddings of text chunks. It enables fast similarity search and retrieval."},

    {"question": "What is FAISS used for?", 
     "expected_answer": "FAISS is a library for efficient similarity search on dense vectors. It allows rapid nearest-neighbor search and is commonly used for embeddings generated by transformer models."},

    {"question": "What is MiniLM?", 
     "expected_answer": "MiniLM is a lightweight sentence transformer model used for generating embeddings from text. It converts sentences or paragraphs into numerical vectors that capture semantic meaning."},

    {"question": "What is FLAN-T5?", 
     "expected_answer": "FLAN-T5 is a text-to-text transformer model that can perform multiple NLP tasks such as summarization, translation, and question answering. It uses an encoder-decoder architecture."},

    {"question": "What is semantic search?", 
     "expected_answer": "Semantic search is the process of finding information based on meaning rather than keyword matching. It relies on embeddings and vector similarity to retrieve relevant content."},

    {"question": "What is document chunking?", 
     "expected_answer": "Document chunking divides large texts into smaller segments. Chunking helps in managing long documents efficiently and improves retrieval in RAG pipelines."},

    {"question": "Why do we use vector databases?", 
     "expected_answer": "Vector databases store embeddings along with metadata, allowing fast similarity search. They support operations like inserting new embeddings, updating, and querying."},

    {"question": "How does a RAG API work?", 
     "expected_answer": "RAG APIs combine document chunking, embeddings stored in a vector DB, and an LLM that generates answers based on retrieved chunks."},

    {"question": "Why use MiniLM in RAG?", 
     "expected_answer": "MiniLM generates embeddings for text chunks, which are used to find semantically similar content quickly in the vector database."},

    {"question": "Why is FLAN-T5 used in this project?", 
     "expected_answer": "FLAN-T5 generates answers based on retrieved context from embeddings, providing accurate and context-aware responses."},

    {"question": "Can FAISS be used for persistent storage?", 
     "expected_answer": "No, FAISS only stores embeddings in memory or on disk indices, but it does not manage document metadata like ChromaDB."},

    {"question": "What is the role of embeddings in RAG?", 
     "expected_answer": "Embeddings convert text chunks into numerical vectors that capture semantic meaning, which are then used for similarity search and retrieval."},

    {"question": "What type of architecture does FLAN-T5 have?", 
     "expected_answer": "FLAN-T5 uses an encoder-decoder architecture."},

    {"question": "What is the benefit of chunking documents?", 
     "expected_answer": "Chunking improves retrieval efficiency and helps the LLM generate better answers by providing relevant context segments."},

    {"question": "How does ChromaDB retrieve relevant chunks?", 
     "expected_answer": "ChromaDB performs similarity search on embeddings to quickly fetch the most relevant document chunks."},

    {"question": "What makes FastAPI suitable for RAG?", 
     "expected_answer": "FastAPI supports asynchronous requests, automatic docs, and fast performance, making it ideal for serving AI APIs."},

    {"question": "What is the difference between FAISS and ChromaDB?", 
     "expected_answer": "FAISS is a library for similarity search on embeddings, whereas ChromaDB is a vector database that stores embeddings along with metadata and allows persistent storage."},

    {"question": "What is the overall goal of a RAG system?", 
     "expected_answer": "The goal is to provide context-aware, accurate answers by combining retrieval of relevant documents with generation from an LLM."}
]

# Initialize components
embedder = Embedder()
retriever = Retriever()
retriever.load_index()
generator = Generator()

# Load raw documents (for context retrieval)
with open("data/sample.txt", "r", encoding="utf-8") as f:
    docs = f.readlines()

results = []

for item in evaluation_data:
    question = item["question"]
    expected = item["expected_answer"]

    # Measure end-to-end time
    start_total = time.time()

    # Step 1: Embed the question
    q_embed = embedder.get_embeddings([question])
    q_embed = np.array(q_embed).astype("float32")

    # Step 2: Measure chunk retrieval time
    start_retrieval = time.time()
    indices, distances = retriever.search(q_embed, k=3)
    retrieval_time = time.time() - start_retrieval

# Step 3: Prepare context
    context = "\n".join([docs[i].strip() for i in indices])


    # Step 4: Generate answer
    answer = generator.generate_answer(context, question)

    end_total = time.time()
    total_time = end_total - start_total

    # Step 5: Calculate answer relevance
    relevance = fuzz.token_set_ratio(answer, expected)

    results.append({
        "question": question,
        "expected": expected,
        "answer": answer,
        "relevance": relevance,
        "retrieval_time_sec": round(retrieval_time, 4),
        "total_time_sec": round(total_time, 4)
    })

# Summary
total_questions = len(results)
avg_relevance = sum(r["relevance"] for r in results) / total_questions
avg_retrieval = sum(r["retrieval_time_sec"] for r in results) / total_questions
avg_total = sum(r["total_time_sec"] for r in results) / total_questions

print(f"Total questions: {total_questions}")
print(f"Average answer relevance: {avg_relevance:.2f}%")
print(f"Average chunk retrieval time: {avg_retrieval:.4f} sec")
print(f"Average end-to-end response time: {avg_total:.4f} sec\n")

# Print detailed results
for r in results:
    print(f"Q: {r['question']}")
    print(f"Expected: {r['expected']}")
    print(f"Answer: {r['answer']}")
    print(f"Relevance: {r['relevance']}%")
    print(f"Chunk retrieval time: {r['retrieval_time_sec']} sec")
    print(f"End-to-end time: {r['total_time_sec']} sec\n")
